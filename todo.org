* DONE Yadav and Michalak (2013) Kronecker products
* TODO Y&M2013 reduced posterior uncertainties
  We don't usually care about uncertainties at a three-hour scale.
  Summing the first axis of B to monthly time scale and reporting that
  should be plenty.  We don't often care about sub-monthly fluxes
  either, but the flexibility can be useful.
** DONE Implement
** TODO Test implementation
* DONE Compare performance for different input chunk-sizes
  Doing everything in memory is only slightly slower, so there should
  be some optimum where caching is still good but I'm not spending so
  long building a huge dask graph and iterating through it.  Around
  4e3 on a side seems decent.  Smaller chunks run out of memory
  creating the task graph, while larger chunks do a bunch of work then
  get stuck in sem_wait and pthread_cond_wait.  Keeping everything in
  memory is a good way to avoid straying from the narrow range where
  dask works.  This is now implemented.
* TODO Investigate nonzero increments for spatial structure
  R packages geostatsp, geoR, fields, and RandomFields would probably
  be useful.  Python package scikit-gstat.  To be clear, this would
  use the "large" increments to try to fit a variogram, and use that
  to iterate.
* DONE Check whether storing spectrum as a numpy array makes HomogeneousIsotropicCorrelations faster
  It may be re-calculating the weights on each trip through, which
  would be really slow.  
  RESULT: If everything is a dask calculation, this
  stays in memory.
* TODO test LinearOperator subclasses with other shapes
  Should this to work?
  Check the scipy-provided implementations:
  find one that doesn't delegate to :meth:`dot` or :meth:`solve`,
  which are designed to handle this case.
  If LinearOperator handles this, code can be simpler.
  If not, expand things to handle this and test that.
** LinearOperator[N, N].dot(array_like[..., N])
   I'm not entirely sure where I'd use this.  What I can use more
   easily is LinearOperator[..., N, N].dot(array_like[N, K])
** DONE LinearOperator[N, N].dot(array_like[N, K])
   B H_T does this, and is tested by the solvers.
   I should write a specific test for this, though.
* TODO Put glue code from example inversion into wrapper module
** TODO Function to align influence functions
*** DONE Implement
*** TODO Test
** TODO Function running inversion given correlation functions et al. 
   Takes prior(s?), influence function, spatial and temporal
   correlation functions for obs and prior errors, and the lower-level
   method to use; turns these in to relevant operators; reshapes
   everything for the inversion, and runs it.
*** TODO Have this also generate and use the reduced B to avoid full A.
    Still need to implement this in the solvers.  For variational
    methods, it's faster to use some other minimizer and have the
    wrapper calculate the errors.  For OI, I already have (HBHT + R)
    calculated in full, so using this directly might be faster.
**** DONE Check whether saving and restoring (HBHT+R) works for a month
     I think the current method may be trying to hold BH^T in memory,
     to avoid extra computation.  Saving and restoring this matrix
     would make it forget it ever knew the part, and recompute it as
     needed.
     RESULT: Kinda.
**** DONE Figure out interface for reduced A
     Var is fairly straightforward (use CG or similar minimizer,
     calculate A from scratch in wrapper), and PSAS is unreliable
     enough that a similar approach may be a good idea.  OI calculates
     (HBH^T + R) in full already, so using this directly is (in
     theory) more accurate.  However, I would then need to pass in
     either two versions for both H and B (inside and outside parens) 
     or three of B (reduced, half reduced, and full).
     $ (I - KH) B = B - B H^T (H B H^T + R)^{-1} H B $
     RESULT: Reduced B and H
* TODO Check whether fancier statistical methods work better.
  Statsmodels StateSpace API might make VARIMA simplish,
  Python Arch package might allow GARCH on top of that.
  Arch also has a few cross-validators built-in.
* TODO Learn about cross-validation and implement it.
* TODO Check whether optimization ideas help
** python3.4.4 cluster_python
   2.75h cpu
   ~82GiB mem
   4.75 wall
** python3.6.6 inversion_environment default numpy 1.14.5
   2.75h cpu
   ~80GiB mem
   115GiB vmem
   3.2h wall
   Second run, with non-nan July observations:
   4.1h cpu
   90GiB mem
   111GiB vmem
   4.25h wall
** DONE optimize YMKronecker product sums
  100x100 and 1000x1000 matrices, 100000x10 test vec
  bn.nansum: 662 ms/loop
  ndarray.sum: 628 ms/loop
  numexpr: 669 ms/loop
  preallocate: 609 ms/loop
  reshape and transpose: 536 ms/loop
** DONE optimize YMKronecker quadratic form
  100x100 and 1000x1000 matrices, 100000x10 test vec
  vec.T @ (op @ vec): 540 ms/loop
  op.quadratic_form(vec): 467 ms/loop
* TODO Rewrite integrators using wrapper and generators
* TODO Get a toepelitz matrix implementation using np.as_strided working
* TODO Implement Multivariate Laplace noise
  Ref: 
    Samuel KotzTomaz J. KozubowskiKrzysztof Podg√≥rski
    The Laplace Distribution and Generalizations
    A Revisit with Applications to Communications, Economics, Engineering, and Finance
    URL: https://link.springer.com/book/10.1007%2F978-1-4612-0173-1#about
    DOI: 10.1007/978-1-4612-0173-1
    ISBN: 978-1-4612-6646-4
    eISBN: 978-1-4612-0173-1
  Chapter seven has an algorithm for generating multivariate
  asymmetric Laplace random variables, which has multivariate
  elliptically symmetric Laplace random variables as a special
  case.  I only need the symmetric case, but the general case
  might be a good addition to scipy.
